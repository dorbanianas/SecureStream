{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVrVt4bFvg_"
      },
      "source": [
        "## Download the CSE-CIC-IDS2018 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install AWS CLI (uncomment and run this cell if you haven't installed it yet)\n",
        "!../venv/bin/pip install awscli --upgrade\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define the directory path\n",
        "directory = \"../data/CSE-CIC-IDS2018\"\n",
        "\n",
        "# Check if the directory is empty\n",
        "if not os.listdir(directory):\n",
        "    # Download data from AWS S3 bucket\n",
        "    subprocess.run([\"aws\", \"s3\", \"sync\", \"--no-sign-request\", \"--recursive\", \"s3://cse-cic-ids2018/Processed Traffic Data for ML Algorithms/\", directory], check=True)\n",
        "    print(\"Data downloaded successfully.\")\n",
        "else:\n",
        "    print(f\"The directory '{directory}' is not empty. Data download skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kTBtr-Ibisb"
      },
      "source": [
        "## Data Exploration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xgqhe0NXYdQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.path.abspath('../'))\n",
        "\n",
        "from lib.helper_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI7S5Y2gXP6X"
      },
      "outputs": [],
      "source": [
        "# if saved dataframe file exists, load\n",
        "# if dataframe isn't saved, load raw csv file and save the dataframe\n",
        "dataframe_file = '../data/flowmeter_dataframe.pkl'\n",
        "exists = os.path.isfile(dataframe_file)\n",
        "if exists:\n",
        "    print('dataframe file exists, loading dataframe...')\n",
        "    df = pd.read_pickle(dataframe_file)\n",
        "    print('dataframe loaded.')\n",
        "else:\n",
        "    directory = 'data/CSE-CIC-IDS2018/'\n",
        "    df = pd.DataFrame()\n",
        "    df = read_clean_combine_csv(directory, df, 'Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv')\n",
        "    # save dataframe to file for future use\n",
        "    pd.to_pickle(df, dataframe_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0BCQndLXP6X",
        "outputId": "ad020a46-d61a-436e-8ccf-e1c8e17f5fd9"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CrVbJevXP6Y",
        "outputId": "df6deca5-0a74-49f0-bdb2-ad942f3a567c"
      },
      "outputs": [],
      "source": [
        "df.memory_usage().sum() / 1024**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsylmoCcXP6Z",
        "outputId": "060c311e-0324-4b03-9fd4-0b4f80fc4551"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNyqCr2DXP6Z"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by=['timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TDAn8yHXP6Z"
      },
      "outputs": [],
      "source": [
        "df = df[df['timestamp'] > pd.to_datetime('2018-01-01')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj99Su3nXP6a",
        "outputId": "a985be44-c98a-47a7-daea-4d7c8ce10d85"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lv3Dd6dXP6a",
        "outputId": "09c46f82-62a8-4b83-c0f2-fe833aa66e89"
      },
      "outputs": [],
      "source": [
        "# get count of each label\n",
        "print(df['label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvGpsD9bXP6a",
        "outputId": "1df49449-e963-47e0-9df1-5c4752bc45a5"
      },
      "outputs": [],
      "source": [
        "# get distribution in of each label\n",
        "print(df['label'].value_counts()/len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hyJaQOTGlrX"
      },
      "source": [
        "## Downsample the dataset to 100K rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "AK69eIZKXP6a",
        "outputId": "a47fecc6-0f4c-4240-f85d-0b5cc6a54f5b"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "dataframe_file = '../data/multiclassification_dataset.pkl'\n",
        "exists = os.path.isfile(dataframe_file)\n",
        "if exists:\n",
        "    print('100k dataframe file exists, loading dataframe...')\n",
        "    df = pd.read_pickle(dataframe_file)\n",
        "    print('dataframe loaded.')\n",
        "else:\n",
        "    # Calculate class counts\n",
        "    class_counts = df['label'].value_counts().to_dict()\n",
        "\n",
        "    # Total desired number of instances\n",
        "    total_instances = 100000\n",
        "\n",
        "    # Calculate the downsampled number of instances for each class\n",
        "    downsampled_counts = {}\n",
        "    for label, count in class_counts.items():\n",
        "        downsampled_counts[label] = min(count, total_instances // len(class_counts))\n",
        "\n",
        "    # Downsample each class to the desired number of instances\n",
        "    downsampled_data = pd.DataFrame()\n",
        "    for label, count in downsampled_counts.items():\n",
        "        if label != 'Infilteration': \n",
        "            class_data = df[df['label'] == label]\n",
        "            downsampled_data = pd.concat([downsampled_data, resample(class_data,\n",
        "                                                                replace=False,\n",
        "                                                                n_samples=count,\n",
        "                                                                random_state=42)])\n",
        "\n",
        "    # Shuffle the downsampled data\n",
        "    downsampled_data = downsampled_data.sample(frac=1, random_state=42)\n",
        "\n",
        "    # Check the total number of instances\n",
        "    print(\"Total number of instances after downsampling:\", len(downsampled_data))\n",
        "    # save dataframe to file for future use\n",
        "    df = downsampled_data.drop(['dst_port', 'protocol', 'timestamp', 'cwe_flag_count'], axis=1)  # Features\n",
        "    df.sort_index(axis=1, inplace=True)\n",
        "    print('The totale number of features is:', len(df.columns))\n",
        "    pd.to_pickle(df, dataframe_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET_1GjcbXP6b",
        "outputId": "898bf47e-124b-443b-ea25-c8b0f3c38b86"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb7FWSooHI2w"
      },
      "source": [
        "## Create the multiclassification model for the threat detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwBQ-XvKaeT4",
        "outputId": "0697c989-73ff-4886-8081-62d646aae4d7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the label encoder to a file\n",
        "label_encoder_file = '../models/label_encoder.joblib'\n",
        "joblib.dump(label_encoder, label_encoder_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBM0BLtfXP6b",
        "outputId": "b32a9df1-f79e-47c9-ef72-db5db6dbfed1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you already have your dataset stored in a pandas DataFrame df\n",
        "# X contains the features and y contains the labels\n",
        "X = df.drop(['label'], axis=1)  # Features\n",
        "y = df['label']                # Labels\n",
        "\n",
        "# Optionally, you can also specify stratification to ensure class balance in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Print the shapes of the train and test sets to verify the split\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq5jrYW3HZBP"
      },
      "source": [
        "### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKWMMIlqXP6c",
        "outputId": "d3d0d5f8-af2a-40b8-d19b-d097b4cba73d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pickle\n",
        "\n",
        "model_file = '../models/rf_classifier.pkl'\n",
        "\n",
        "exists = os.path.isfile(model_file)\n",
        "\n",
        "if exists:\n",
        "    print('Random Forest model exists, loading model...')\n",
        "    rf_classifier = pickle.load(open(model_file, 'rb'))\n",
        "    print('model loaded.')\n",
        "else:\n",
        "    print('Random Forest model does not exist, training model...')\n",
        "    # Create a Random Forest classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=300, random_state=42)\n",
        "\n",
        "    # Train the classifier on the training data\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    print('model trained.')\n",
        "    # save model to file for future use\n",
        "    pickle.dump(rf_classifier, open(model_file, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "qIyPOQEM0pxn",
        "outputId": "240cb9d7-c163-4df9-ef43-b55aaaeda162"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "\n",
        "df_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by feature importance in descending order\n",
        "df_importance_sorted = df_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance', y='Feature', data=df_importance_sorted)\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ateyAY6XHws4"
      },
      "source": [
        "### Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pcQ4b_62Wa_",
        "outputId": "e27d6acd-9d68-4040-a3dc-ed1bb6c96386"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have already split your data into X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = len(set(y_train))\n",
        "\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(input_shape, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYscM3ZIAwb"
      },
      "source": [
        "### XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAvsWQIGaIQs",
        "outputId": "39bd55cd-db95-4c44-ef57-427b97ac30bf"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming you have already split your data into X_train, X_test, y_train, y_test\n",
        "# Assuming num_classes is already defined\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'subsample': [0.5, 0.7, 1]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2)\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best estimator\n",
        "best_xgb_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0GNjV4DIOVq"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G9feutkXP6d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have already split your data into X_train, X_test, y_train, y_test\n",
        "\n",
        "# Initialize the Logistic Regression classifier\n",
        "logistic_regression = LogisticRegression(C=0.001, max_iter=1000, solver='saga', penalty='l1', verbose=1)  # Increase max_iter if needed\n",
        "\n",
        "# Train the classifier on the training data\n",
        "logistic_regression.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
